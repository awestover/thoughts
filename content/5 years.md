> Q: How, if at all does [[the objective]] change if you estimate Pr that humans (in particular me) are extinct before 2030 is larger than 10 percent?

- Well, this would mean that a lot of long term goals do not matter at all. 
- For instance, "curing aging" would no longer be such an important thing to do. 
- It seems to make "accumulate money" a pretty obviously bad strategy. This can help cure an aversion to allocating money to wherever you think it should go in order to maximize utility. 
- One other update is that you should try to do some things to lower x-risk.  Should you drop everything and just work on reducing x-risk? I mean, maybe. I'm not sure.
- I think it makes it more obvious that you shouldn't engage in trade-offs of "pain now" for nebulous future benefits. 

However, the reality is, even if you didn't think this, you'd be a fool to assign Pr(yourself dying in the next 50 years) < 10\%.
Heck, Pr(extinction) per year has probably been pretty high since the end of WW2. 
[[nukes xrisk thoughts]]

That is to say, this isn't a completely novel problem.

> Q: Is that supposed to make it better somehow? \
> A: no, although it is kind of saying that you can't blame the whole problem on just AI. 

Anyways, this reminds me of a quote from [the greatest literary work of 2024](https://nathan-sheffield.github.io/jekyll/update/2024/04/23/faust-reconstructed.html)
> "The proof is left to the reader as an exercise. The battle is worth it, Johann."

To explain this quote I will just give a Brandon Sanderson quote:
> Things only have the value we give to them. And likewise, actions can be worth whatever we decide them to be worth.

Well, if we have one decade, or one week, or even just one day. Then I'd advocate for filling the day with good conscious experiences in line with [[the objective]].  